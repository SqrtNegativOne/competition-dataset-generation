{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8255acf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74580ee",
   "metadata": {},
   "source": [
    "# DataFrame Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcaf7517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 33333\n"
     ]
    }
   ],
   "source": [
    "SEED = 42 # answer to everything\n",
    "np.random.seed(SEED)\n",
    "\n",
    "TRAIN_FRAC = 0.3\n",
    "PUBLIC_LEADERBOARD_FRAC = 0.2\n",
    "PRIVATE_LEADERBOARD_FRAC = 0.5\n",
    "assert TRAIN_FRAC + PUBLIC_LEADERBOARD_FRAC + PRIVATE_LEADERBOARD_FRAC == 1, \"Fractions must sum to 1\"\n",
    "N_TRAIN_SAMPLES = 10000\n",
    "N_SAMPLES = int(N_TRAIN_SAMPLES // TRAIN_FRAC)\n",
    "print(f\"Total number of samples: {N_SAMPLES}\")\n",
    "\n",
    "PUBLIC_PRIVATE_SPLIT = 0.3 # Fraction of samples used for the public dataset\n",
    "TEST_TRAIN_SPLIT = 0.2 # Fraction of samples used for the test set\n",
    "\n",
    "ID_VARIABLE_NAME = \"ID\" # Name of the ID variable.\n",
    "TARGET_VARIABLE_NAME = \"y\" # Name of the target variable.\n",
    "\n",
    "INITIAL_NOISE = 0.1 # Previously was 0.3\n",
    "FINAL_NOISE = 0.05 # Added to the final target\n",
    "NAN_PROB = 0.1 # Probability of a value being NaN\n",
    "\n",
    "DATA_DIR = \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "655d6d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import (\n",
    "    make_moons, make_circles, make_blobs, make_classification, make_hastie_10_2,\n",
    "    make_friedman1, make_friedman2, make_friedman3, make_regression\n",
    ")\n",
    "from typing import Callable\n",
    "\n",
    "class ds:\n",
    "    def __init__(self, weight: float, name: str, generator: Callable[[], tuple[np.ndarray, np.ndarray]]):\n",
    "        self.weight: float = weight\n",
    "        self.name: str = name\n",
    "\n",
    "        self.raw_X: np.ndarray\n",
    "        self.raw_y: np.ndarray\n",
    "        self.raw_X, self.raw_y = generator()\n",
    "        self.X: pd.DataFrame = pd.DataFrame(\n",
    "            self.raw_X,\n",
    "            columns=[\n",
    "                f\"{self.name}_{i+1}\" for i in range(self.raw_X.shape[1])\n",
    "            ]\n",
    "        )\n",
    "        self.y: pd.Series = pd.Series(self.raw_y, name=\"{self.name}_y\")\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(self.name)\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return isinstance(other, ds) and self.name == other.name\n",
    "\n",
    "# Make sure each has a unique name.\n",
    "DATASETS: list[ds] = [\n",
    "    ds(4, \"moon\",      lambda: make_moons(n_samples=N_SAMPLES, noise=INITIAL_NOISE, random_state=SEED)),\n",
    "    ds(3, \"circle\",    lambda: make_circles(n_samples=N_SAMPLES, noise=INITIAL_NOISE, factor=0.6, random_state=SEED)),\n",
    "    ds(2, \"blob\",      lambda: make_blobs(n_samples=N_SAMPLES, centers=3, n_features=2, random_state=SEED, return_centers=False)), # type: ignore # return_centers=False to avoid returning centers\n",
    "    ds(2, \"hastie\",    lambda: make_hastie_10_2(n_samples=N_SAMPLES, random_state=SEED)),\n",
    "    ds(2, \"friedman1\", lambda: make_friedman1(n_samples=N_SAMPLES, noise=INITIAL_NOISE, random_state=SEED)),\n",
    "    ds(2, \"friedman2\", lambda: make_friedman2(n_samples=N_SAMPLES, noise=INITIAL_NOISE, random_state=SEED)),\n",
    "    ds(2, \"friedman3\", lambda: make_friedman3(n_samples=N_SAMPLES, noise=INITIAL_NOISE, random_state=SEED)),\n",
    "    ds(1, \"class\",     lambda: make_classification(n_samples=N_SAMPLES, n_features=5, n_informative=3, n_redundant=1, random_state=SEED)),\n",
    "    ds(1, \"reg\",       lambda: make_regression(n_samples=N_SAMPLES, n_features=5, n_informative=3, noise=INITIAL_NOISE, random_state=SEED, coef=False)), # type: ignore # coef=False to avoid returning coefficients\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04398806",
   "metadata": {},
   "outputs": [],
   "source": [
    "id = pd.Series(np.arange(N_SAMPLES), name=ID_VARIABLE_NAME)\n",
    "df = pd.concat([id] + [d.X for d in DATASETS], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1014c928",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_weights = np.array([d.weight for d in DATASETS], dtype=np.float64) # dtype=np.float64 for division in the next line\n",
    "normalized_weights /= normalized_weights.sum()\n",
    "y_final = sum(DATASETS[i].y * normalized_weights[i] for i in range(len(DATASETS)))\n",
    "y_final += np.random.normal(0, FINAL_NOISE, size= N_SAMPLES) # Some small, random noise to the final target\n",
    "df[TARGET_VARIABLE_NAME] = y_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d00422",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1081aa7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9a7c5bf",
   "metadata": {},
   "source": [
    "# CSV Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ef789a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make some cells NaN with a given probability\n",
    "\n",
    "n_cells = df.size\n",
    "n_nan = int(n_cells * 0.10)\n",
    "\n",
    "# Randomly choose positions\n",
    "nan_indices = (\n",
    "    np.random.choice(df.index, n_nan, replace=True),\n",
    "    np.random.choice(df.columns, n_nan, replace=True)\n",
    ")\n",
    "\n",
    "# Assign NaN\n",
    "for row, col in zip(*nan_indices):\n",
    "    df.loc[row, col] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42558069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle columns, keeping the target column at the end.\n",
    "cols = df.columns.tolist()\n",
    "df = df[np.random.permutation(cols[:-1]).tolist() + [cols[-1]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ec63584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Renaming step. The names should fit the theme. After renaming, some columns can be Zalgo'd. Currently all of them are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4fb625c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zalgo the column names. Kinda evil but it adds a bit of fun to the dataset.\n",
    "\n",
    "import random\n",
    "\n",
    "# Zalgo character set\n",
    "zalgo_up = [chr(c) for c in range(0x0300, 0x036F)]\n",
    "zalgo_mid = [chr(c) for c in range(0x1AB0, 0x1AFF)]\n",
    "zalgo_down = [chr(c) for c in range(0x1DC0, 0x1DFF)]\n",
    "zalgo_chars = zalgo_up + zalgo_mid + zalgo_down\n",
    "\n",
    "def zalgoify(text: str, intensity: float = 0.5, max_marks: int = 3) -> str:\n",
    "    \"\"\"\n",
    "    intensity: 0.0 (no zalgo) â†’ 1.0 (every char zalgo'd)\n",
    "    max_marks: maximum number of diacritic marks per affected character\n",
    "    \"\"\"\n",
    "    zalgo_text = \"\"\n",
    "    for char in text:\n",
    "        zalgo_text += char\n",
    "        if random.random() < intensity:\n",
    "            for _ in range(random.randint(1, max_marks)):\n",
    "                zalgo_text += random.choice(zalgo_chars)\n",
    "    return zalgo_text\n",
    "\n",
    "# Apply zalgo to the column names\n",
    "df.columns = [zalgoify(col) for col in df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4073b9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_PUBLIC_SAMPLES = int(N_SAMPLES * PUBLIC_LEADERBOARD_FRAC)\n",
    "train = df.iloc[:N_TRAIN_SAMPLES]\n",
    "public = df.iloc[N_TRAIN_SAMPLES:N_TRAIN_SAMPLES + N_PUBLIC_SAMPLES]\n",
    "private = df.iloc[N_TRAIN_SAMPLES + N_PUBLIC_SAMPLES:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8f23caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the datasets to CSV files\n",
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime(\"%d_%H-%M-%S\")\n",
    "\n",
    "train.to_csv(f\"{DATA_DIR}/train_{timestamp}.csv\", index=ID_VARIABLE_NAME)\n",
    "public.to_csv(f\"{DATA_DIR}/public_{timestamp}.csv\", index=ID_VARIABLE_NAME)\n",
    "private.to_csv(f\"{DATA_DIR}/private_{timestamp}.csv\", index=ID_VARIABLE_NAME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
