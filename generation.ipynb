{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8255acf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74580ee",
   "metadata": {},
   "source": [
    "# Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bcaf7517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Private dataset: 70.000000%\n",
      "Public train dataset: 24.000000%\n",
      "Public test dataset: 6.000000%\n",
      "Public train dataset size: 7200\n"
     ]
    }
   ],
   "source": [
    "SEED = 42 # answer to everything\n",
    "np.random.seed(SEED)\n",
    "\n",
    "N_SAMPLES = 30000\n",
    "PUBLIC_PRIVATE_SPLIT = 0.3 # Fraction of samples used for the public dataset\n",
    "TEST_TRAIN_SPLIT = 0.2 # Fraction of samples used for the test set\n",
    "print(f\"Private dataset: {(1 - PUBLIC_PRIVATE_SPLIT) * 100:1f}%\\nPublic train dataset: {PUBLIC_PRIVATE_SPLIT * (1 - TEST_TRAIN_SPLIT) * 100:1f}%\\nPublic test dataset: {PUBLIC_PRIVATE_SPLIT * TEST_TRAIN_SPLIT * 100:1f}%\")\n",
    "print(f\"Public train dataset size: {int(N_SAMPLES * PUBLIC_PRIVATE_SPLIT * (1 - TEST_TRAIN_SPLIT))}\")\n",
    "\n",
    "INITIAL_NOISE = 0.1 # Previously was 0.3\n",
    "FINAL_NOISE = 0.05 # Added to the final target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655d6d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import (\n",
    "    make_moons, make_circles, make_blobs, make_classification, make_hastie_10_2,\n",
    "    make_friedman1, make_friedman2, make_friedman3, make_regression\n",
    ")\n",
    "from typing import Callable\n",
    "\n",
    "class ds:\n",
    "    def __init__(self, weight: float, name: str, generator: Callable[[], tuple[np.ndarray, np.ndarray]]):\n",
    "        self.weight: float = weight\n",
    "        self.name: str = name\n",
    "\n",
    "        self.raw_X: np.ndarray\n",
    "        self.raw_y: np.ndarray\n",
    "        self.raw_X, self.raw_y = generator()\n",
    "        self.X: pd.DataFrame = pd.DataFrame(\n",
    "            self.raw_X,\n",
    "            columns=[\n",
    "                f\"{self.name}_{i+1}\" for i in range(self.raw_X.shape[1])\n",
    "            ]\n",
    "        )\n",
    "        self.y: pd.Series = pd.Series(self.raw_y, name=\"{self.name}_y\")\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(self.name)\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return isinstance(other, ds) and self.name == other.name\n",
    "\n",
    "# Make sure each has a unique name.\n",
    "DATASETS: list[ds] = [\n",
    "    ds(4, \"moon\",      lambda: make_moons(n_samples=N_SAMPLES, noise=INITIAL_NOISE, random_state=SEED)),\n",
    "    ds(3, \"circle\",    lambda: make_circles(n_samples=N_SAMPLES, noise=INITIAL_NOISE, factor=0.6, random_state=SEED)),\n",
    "    ds(2, \"blob\",      lambda: make_blobs(n_samples=N_SAMPLES, centers=3, n_features=2, random_state=SEED, return_centers=False)), # type: ignore # return_centers=False to avoid returning centers\n",
    "    ds(2, \"hastie\",    lambda: make_hastie_10_2(n_samples=N_SAMPLES, random_state=SEED)),\n",
    "    ds(2, \"friedman1\", lambda: make_friedman1(n_samples=N_SAMPLES, noise=INITIAL_NOISE, random_state=SEED)),\n",
    "    ds(2, \"friedman2\", lambda: make_friedman2(n_samples=N_SAMPLES, noise=INITIAL_NOISE, random_state=SEED)),\n",
    "    ds(2, \"friedman3\", lambda: make_friedman3(n_samples=N_SAMPLES, noise=INITIAL_NOISE, random_state=SEED)),\n",
    "    ds(1, \"class\",     lambda: make_classification(n_samples=N_SAMPLES, n_features=5, n_informative=3, n_redundant=1, random_state=SEED)),\n",
    "    ds(1, \"reg\",       lambda: make_regression(n_samples=N_SAMPLES, n_features=5, n_informative=3, noise=INITIAL_NOISE, random_state=SEED, coef=False)), # type: ignore # coef=False to avoid returning coefficients\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04398806",
   "metadata": {},
   "outputs": [],
   "source": [
    "id = pd.Series(np.arange(N_SAMPLES), name=\"id\")\n",
    "df = pd.concat(id + [d.X for d in DATASETS], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1014c928",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_weights = np.array([d.weight for d in DATASETS], dtype=np.float64) # dtype=np.float64 for division in the next line\n",
    "normalized_weights /= normalized_weights.sum()\n",
    "y_final = sum(DATASETS[i].y * normalized_weights[i] for i in range(len(DATASETS)))\n",
    "y_final += np.random.normal(0, FINAL_NOISE, size= N_SAMPLES) # Some small, random noise to the final target\n",
    "df['y'] = y_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d00422",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1081aa7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9a7c5bf",
   "metadata": {},
   "source": [
    "# CSV Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fb625c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to data_09_19-06-54.csv\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime(\"%d_%H-%M-%S\")\n",
    "\n",
    "# Shuffle columns, keeping the target column at the end.\n",
    "cols = df.columns.tolist()\n",
    "df = df[np.random.permutation(cols[:-1]).tolist() + [cols[-1]]]\n",
    "\n",
    "# TODO: Renaming step.\n",
    "\n",
    "# Split into 3 datasets: public train, public test, private\n",
    "public_train_size = int(N_SAMPLES * PUBLIC_PRIVATE_SPLIT * (1 - TEST_TRAIN_SPLIT))\n",
    "public_test_size = N_SAMPLES - public_train_size\n",
    "\n",
    "df_public_train = df.iloc[:public_train_size]\n",
    "df_public_test = df.iloc[public_train_size:public_train_size + public_test_size]\n",
    "df_private = df.iloc[public_train_size + public_test_size:]\n",
    "\n",
    "df_public_train.to_csv(f\"public_train_{timestamp}.csv\", index=False)\n",
    "df_public_test.to_csv(f\"public_test_{timestamp}.csv\", index=False)\n",
    "df_private.to_csv(f\"private_{timestamp}.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
