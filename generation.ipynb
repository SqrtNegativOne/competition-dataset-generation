{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8255acf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74580ee",
   "metadata": {},
   "source": [
    "# DataFrame Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaf7517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 33333\n"
     ]
    }
   ],
   "source": [
    "SEED = 42 # answer to everything\n",
    "# Don't really need reproducibility rn.\n",
    "# np.random.seed(SEED)\n",
    "# random.seed(SEED)\n",
    "\n",
    "TRAIN_FRAC = 0.3\n",
    "PUBLIC_LEADERBOARD_FRAC = 0.2\n",
    "PRIVATE_LEADERBOARD_FRAC = 0.5\n",
    "assert TRAIN_FRAC + PUBLIC_LEADERBOARD_FRAC + PRIVATE_LEADERBOARD_FRAC == 1, \"Fractions must sum to 1\"\n",
    "N_TRAIN_SAMPLES = 10000\n",
    "N_SAMPLES = int(N_TRAIN_SAMPLES // TRAIN_FRAC)\n",
    "print(f\"Total number of samples: {N_SAMPLES}\")\n",
    "\n",
    "PUBLIC_PRIVATE_SPLIT = 0.3 # Fraction of samples used for the public dataset\n",
    "TEST_TRAIN_SPLIT = 0.2 # Fraction of samples used for the test set\n",
    "\n",
    "ID_VARIABLE_NAME = \"ID\" # Name of the ID variable.\n",
    "TARGET_VARIABLE_NAME = \"y\" # Name of the target variable.\n",
    "\n",
    "INITIAL_NOISE = 0.1 # Previously was 0.3\n",
    "FINAL_NOISE = 0.05 # Added to the final target\n",
    "NAN_PROB = 0.1 # Probability of a value being NaN\n",
    "\n",
    "DATA_DIR = \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391e9788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cat_cols():\n",
    "    \"\"\"Generates categorical columns with random values.\"\"\"\n",
    "    # Nominal\n",
    "    types = ['jut', 'kiv', 'tir', 'vel', 'qou']\n",
    "    types_p = [0.2, 0.3, 0.3, 0.2, 0.1]\n",
    "    type_col = np.random.choice(types, N_SAMPLES, p=types_p)\n",
    "\n",
    "    # Ordinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655d6d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import (\n",
    "    make_moons, make_circles, make_blobs, make_classification, make_hastie_10_2,\n",
    "    make_friedman1, make_friedman2, make_friedman3, make_regression\n",
    ")\n",
    "from typing import Callable\n",
    "\n",
    "class ds:\n",
    "    def __init__(self, weight: float, name: str, generator: Callable[[], tuple[np.ndarray, np.ndarray]]):\n",
    "        self.weight: float = weight\n",
    "        self.name: str = name\n",
    "\n",
    "        self.raw_X: np.ndarray\n",
    "        self.raw_y: np.ndarray\n",
    "        self.raw_X, self.raw_y = generator()\n",
    "        self.X: pd.DataFrame = pd.DataFrame(\n",
    "            self.raw_X,\n",
    "            columns=[\n",
    "                f\"{self.name}_{i+1}\" for i in range(self.raw_X.shape[1])\n",
    "            ]\n",
    "        )\n",
    "        self.y: pd.Series = pd.Series(self.raw_y, name=\"{self.name}_y\")\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(self.name)\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return isinstance(other, ds) and self.name == other.name\n",
    "\n",
    "# Make sure each has a unique name.\n",
    "DATASETS: list[ds] = [\n",
    "    ds(4, \"moon\",      lambda: make_moons(n_samples=N_SAMPLES, noise=INITIAL_NOISE, random_state=SEED)),\n",
    "    ds(3, \"circle\",    lambda: make_circles(n_samples=N_SAMPLES, noise=INITIAL_NOISE, factor=0.6, random_state=SEED)),\n",
    "    ds(2, \"blob\",      lambda: make_blobs(n_samples=N_SAMPLES, centers=3, n_features=2, random_state=SEED, return_centers=False)), # type: ignore # return_centers=False to avoid returning centers\n",
    "    ds(2, \"hastie\",    lambda: make_hastie_10_2(n_samples=N_SAMPLES, random_state=SEED)),\n",
    "    ds(2, \"friedman1\", lambda: make_friedman1(n_samples=N_SAMPLES, noise=INITIAL_NOISE, random_state=SEED)),\n",
    "    ds(2, \"friedman2\", lambda: make_friedman2(n_samples=N_SAMPLES, noise=INITIAL_NOISE, random_state=SEED)),\n",
    "    ds(2, \"friedman3\", lambda: make_friedman3(n_samples=N_SAMPLES, noise=INITIAL_NOISE, random_state=SEED)),\n",
    "    ds(2, \"type\",      make_cat_cols),\n",
    "    ds(1, \"class\",     lambda: make_classification(n_samples=N_SAMPLES, n_features=5, n_informative=3, n_redundant=1, random_state=SEED)),\n",
    "    ds(1, \"reg\",       lambda: make_regression(n_samples=N_SAMPLES, n_features=5, n_informative=3, noise=INITIAL_NOISE, random_state=SEED, coef=False)), # type: ignore # coef=False to avoid returning coefficients\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1014c928",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([d.X for d in DATASETS], axis=1)\n",
    "normalized_weights = np.array([d.weight for d in DATASETS], dtype=np.float64) # dtype=np.float64 for division in the next line\n",
    "normalized_weights /= normalized_weights.sum()\n",
    "\n",
    "y_final = sum(DATASETS[i].y * normalized_weights[i] for i in range(len(DATASETS)))\n",
    "y_final += np.random.normal(0, FINAL_NOISE, size= N_SAMPLES) # Some small, random noise to the final target\n",
    "df[TARGET_VARIABLE_NAME] = y_final\n",
    "\n",
    "id = pd.Series(np.arange(N_SAMPLES), name=ID_VARIABLE_NAME)\n",
    "df[ID_VARIABLE_NAME] = id\n",
    "# Making sure the ID column is the first column; makes the analysis prettier.\n",
    "cols = [ID_VARIABLE_NAME] + [col for col in df.columns if col != ID_VARIABLE_NAME]\n",
    "df = df[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d00422",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1081aa7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9a7c5bf",
   "metadata": {},
   "source": [
    "# CSV Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ef789a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make some cells NaN with a given probability\n",
    "id = df[ID_VARIABLE_NAME]\n",
    "df.drop(columns=[ID_VARIABLE_NAME], inplace=True)\n",
    "n_cells = df.size\n",
    "n_nan = int(n_cells * 0.10)\n",
    "\n",
    "# Randomly choose positions\n",
    "nan_indices = (\n",
    "    np.random.choice(df.index, n_nan, replace=True),\n",
    "    np.random.choice(df.columns, n_nan, replace=True)\n",
    ")\n",
    "\n",
    "# Assign NaN\n",
    "for row, col in zip(*nan_indices):\n",
    "    df.loc[row, col] = np.nan\n",
    "\n",
    "df = pd.concat([id, df], axis=1)  # Reattach the ID column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42558069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle columns, keeping the target column at the end and ID at the beginning\n",
    "cols = df.columns.tolist()\n",
    "df = df[[cols[0]] + np.random.permutation(cols[1:-1]).tolist() + [cols[-1]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ec63584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thematic renaming of columns\n",
    "renaming = {\n",
    "    ID_VARIABLE_NAME: 'LOCAL_IDENTIFIER',\n",
    "    TARGET_VARIABLE_NAME: 'CORRUCYSTIC_DENSITY',\n",
    "\n",
    "    'moon_1': 'v0rt3X',\n",
    "    'moon_2': 'v1rt3X',\n",
    "    'circle_1': 'r1Ng',\n",
    "    'circle_2': 'r2Ng',\n",
    "    'blob_1': 'b1oRb13',\n",
    "    'blob_2': 'b2oRb13',\n",
    "}\n",
    "\n",
    "generic_cols = [col for col in df.columns if col not in renaming]\n",
    "\n",
    "import string\n",
    "\n",
    "def random_gibberish(mean_length: int = 4, std_length = 3) -> str:\n",
    "    length = max(3, int(round(random.gauss(mean_length, std_length))))\n",
    "    chars = string.ascii_letters + string.digits + string.punctuation\n",
    "    return ''.join(random.choice(chars) for _ in range(length))\n",
    "\n",
    "gibberish_mapping = {col: random_gibberish() for col in generic_cols}\n",
    "\n",
    "final_mapping = {**renaming, **gibberish_mapping}\n",
    "\n",
    "df.rename(columns=final_mapping, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4073b9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_PUBLIC_SAMPLES = int(N_SAMPLES * PUBLIC_LEADERBOARD_FRAC)\n",
    "train = df.iloc[:N_TRAIN_SAMPLES]\n",
    "public = df.iloc[N_TRAIN_SAMPLES:N_TRAIN_SAMPLES + N_PUBLIC_SAMPLES]\n",
    "private = df.iloc[N_TRAIN_SAMPLES + N_PUBLIC_SAMPLES:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8f23caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the datasets to CSV files\n",
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime(\"%d_%H-%M-%S\")\n",
    "\n",
    "train.to_csv(f\"{DATA_DIR}/{timestamp}_train.csv\")\n",
    "public.to_csv(f\"{DATA_DIR}/{timestamp}_public.csv\")\n",
    "private.to_csv(f\"{DATA_DIR}/{timestamp}_private.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
